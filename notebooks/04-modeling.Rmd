---
title: "04 - Modeling"
output: html_notebook
---

### Imports

```{r}
library(dplyr)
library(ggplot2)
library(caret)
```

```{r}
train_set <- readRDS("../data/train_set.rds")
head(train_set)
```

```{r}
test_set <- readRDS("../data/test_set.rds")
head(test_set)
```

```{r}
train_set_rfe <- readRDS("../data/train_set_rfe.rds")
head(train_set_rfe)
```

```{r}
test_set_rfe <- readRDS("../data/test_set_rfe.rds")
head(test_set_rfe)
```

### Modeling

Define repeated 10-fold cross validation.

```{r}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
```

Define a helper function to train model.

```{r}
train_model <- function(method = "",
                        data = train_set,
                        tuneGrid = NULL,
                        tuneLength = 3) {
  set.seed(42)

  fit <- train(
    Adaptivity.Level ~ .,
    data = data,
    method = method,
    trControl = fit_control,
    tuneGrid = tuneGrid,
    tuneLength = tuneLength
  )

  print(fit)
  
  return(fit)
}
```

Define a helper function to evaluate model.

```{r}
evaluate_model <- function(
    model = NULL, 
    alias = "",
    data = test_set,
    family = "") {
  
  pred <- predict(model, data)

  cm <- confusionMatrix(pred, data$Adaptivity.Level, mode = "prec_recall")
  print(cm)
  
  return(data.frame(
    Model = alias,
    Family = ifelse(family == "", alias, family),
    Accuracy = round(cm$overall[["Accuracy"]], 3),
    Precision.Low = round(cm$byClass["Class: Low", "Precision"], 3),
    Recall.Low = round(cm$byClass["Class: Low", "Recall"], 3),
    F1.Low = round(cm$byClass["Class: Low", "F1"], 3),
    Precision.Moderate = round(cm$byClass["Class: Moderate", "Precision"], 3),
    Recall.Moderate = round(cm$byClass["Class: Moderate", "Recall"], 3),
    F1.Moderate = round(cm$byClass["Class: Moderate", "F1"], 3),
    Precision.High = round(cm$byClass["Class: High", "Precision"], 3),
    Recall.High = round(cm$byClass["Class: High", "Recall"], 3),
    F1.High = round(cm$byClass["Class: High", "F1"], 3)
  ))
}
```

#### Logistic Regression

##### Train Set

```{r}
lr_clf <- train_model("glmnet")
```

```{r}
plot(lr_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
lr_clf <- train_model("glmnet", tuneLength = 10)
```

```{r}
plot(lr_clf)
```

Tuning done.

```{r}
lr_metrics <- evaluate_model(lr_clf, "LR")
```

##### Train Set (RFE)

```{r}
lr_clf_rfe <- train_model("glmnet", train_set_rfe)
```

```{r}
plot(lr_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
lr_clf_rfe <- train_model("glmnet", train_set_rfe, tuneLength = 10)
```

```{r}
plot(lr_clf_rfe)
```

Tuning done.

```{r}
lr_metrics_rfe <- evaluate_model(lr_clf_rfe, "LR", test_set_rfe)
```

#### K-Nearest Neighbors

##### Train Set

```{r}
knn_clf <- train_model("knn")
```

```{r}
plot(knn_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
knn_tuneGrid <- expand.grid(k = 1:5)

knn_clf <- train_model("knn", tuneGrid = knn_tuneGrid)
```

```{r}
plot(knn_clf)
```

Tuning done.

```{r}
knn_metrics <- evaluate_model(knn_clf, "KNN")
```

##### Train Set (RFE)

```{r}
knn_clf_rfe <- train_model("knn", train_set_rfe)
```

```{r}
plot(knn_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
knn_tuneGrid_rfe <- expand.grid(k = 1:5)

knn_clf_rfe <- train_model("knn", train_set_rfe, tuneGrid = knn_tuneGrid_rfe)
```

```{r}
plot(knn_clf_rfe)
```

Tuning done.

```{r}
knn_metrics_rfe <- evaluate_model(knn_clf_rfe, "KNN", test_set_rfe)
```

#### Naive Bayes

##### Train Set

```{r}
nb_clf <- train_model("naive_bayes")
```

```{r}
plot(nb_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
nb_tuneGrid <- expand.grid(
  adjust = 1,
  usekernel = TRUE,
  laplace = seq(0.1, 1, 0.1)
)

nb_clf <- train_model("naive_bayes", tuneGrid = nb_tuneGrid)
```

```{r}
plot(nb_clf)
```

Tuning done.

```{r}
nb_metrics <- evaluate_model(nb_clf, "NB")
```

##### Train Set (RFE)

```{r}
nb_clf_rfe <- train_model("naive_bayes", train_set_rfe)
```

```{r}
plot(nb_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
nb_tuneGrid_rfe <- expand.grid(
  adjust = 1,
  usekernel = TRUE,
  laplace = seq(0.1, 1, 0.1)
)

nb_clf_rfe <- train_model(
  "naive_bayes", 
  train_set_rfe, 
  tuneGrid = nb_tuneGrid_rfe
)
```

```{r}
plot(nb_clf_rfe)
```

Tuning done.

```{r}
nb_metrics_rfe <- evaluate_model(nb_clf_rfe, "NB", test_set_rfe)
```

#### Decision Tree

##### Train Set

```{r}
dt_clf <- train_model("rpart2")
```

```{r}
plot(dt_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
dt_clf <- train_model("rpart2", tuneLength = 10)
```

```{r}
plot(dt_clf)
```

Tuning done.

```{r}
dt_metrics <- evaluate_model(dt_clf, "DT")
```

##### Train Set (RFE)

```{r}
dt_clf_rfe <- train_model("rpart2", train_set_rfe)
```

```{r}
plot(dt_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
dt_clf_rfe <- train_model("rpart2", train_set_rfe, tuneLength = 10)
```

```{r}
plot(dt_clf_rfe)
```

Tuning done.

```{r}
dt_metrics_rfe <- evaluate_model(dt_clf_rfe, "DT", test_set_rfe)
```

#### Random Forest

##### Train Set

```{r}
rf_clf <- train_model("ranger")
```

```{r}
plot(rf_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
rf_clf <- train_model("ranger", tuneLength = 10)
```

```{r}
plot(rf_clf)
```

Tuning done.

```{r}
rf_metrics <- evaluate_model(rf_clf, "RF")
```

##### Train Set (RFE)

```{r}
rf_clf_rfe <- train_model("ranger", train_set_rfe)
```

```{r}
plot(rf_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
rf_clf_rfe <- train_model("ranger", train_set_rfe, tuneLength = 10)
```

```{r}
plot(rf_clf_rfe)
```

Tuning done.

```{r}
rf_metrics_rfe <- evaluate_model(rf_clf_rfe, "RF", test_set_rfe)
```

#### XGBoost (Tree)

Define a helper function to train XGBoost model with less verbose output.

```{r}
train_xgb <- function(method = "",
                        data = train_set,
                        tuneGrid = NULL,
                        tuneLength = 3) {
  set.seed(42)

  fit <- train(
    Adaptivity.Level ~ .,
    data = data,
    method = method,
    trControl = fit_control,
    tuneGrid = tuneGrid,
    tuneLength = tuneLength,
    verbosity = 0
  )

  print(fit)
  
  return(fit)
}
```

##### Train Set

```{r}
xgb_tree_clf <- train_xgb("xgbTree")
```

```{r}
plot(xgb_tree_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_tree_tuneGrid <- expand.grid(
  gamma = 0,
  min_child_weight = 1,
  max_depth = 3,
  eta = 0.4,
  subsample = seq(0.5, 1, 0.25),
  colsample_bytree = 0.8,
  nrounds = seq(100, 500, 10)
)

xgb_tree_clf <- train_xgb("xgbTree", tuneGrid = xgb_tree_tuneGrid)
```

```{r}
plot(xgb_tree_clf)
```

Tuning done.

```{r}
xgb_tree_metrics <- evaluate_model(xgb_tree_clf, "XGB - Tree", family = "XGB")
```

##### Train Set (RFE)

```{r}
xgb_tree_clf_rfe <- train_xgb("xgbTree", train_set_rfe)
```

```{r}
plot(xgb_tree_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_tree_tuneGrid_rfe <- expand.grid(
  gamma = 0,
  min_child_weight = 1,
  max_depth = 3,
  eta = 0.4,
  subsample = seq(0.5, 1, 0.25),
  colsample_bytree = 0.8,
  nrounds = seq(100, 500, 10)
)

xgb_tree_clf_rfe <- train_xgb(
  "xgbTree",  
  train_set_rfe,
  tuneGrid = xgb_tree_tuneGrid_rfe
)
```

```{r}
plot(xgb_tree_clf_rfe)
```

Tuning done.

```{r}
xgb_tree_metrics_rfe <- evaluate_model(
  xgb_tree_clf_rfe, 
  "XGB - Tree", 
  test_set_rfe,
  "XGB"
)
```

#### XGBoost (Linear)

##### Train Set

```{r}
xgb_lin_clf <- train_xgb("xgbLinear")
```

```{r}
plot(xgb_lin_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_lin_tuneGrid <- expand.grid(
  eta = 0.3,
  lambda = c(1e-04, 1e-01, 0),
  alpha = 1e-04,
  nrounds = seq(100, 200, 10)
)

xgb_lin_clf <- train_xgb("xgbLinear", tuneGrid = xgb_lin_tuneGrid)
```

```{r}
plot(xgb_lin_clf)
```

Tuning done.

```{r}
xgb_lin_metrics <- evaluate_model(xgb_lin_clf, "XGB - Linear", family = "XGB")
```

##### Train Set (RFE)

```{r}
xgb_lin_clf_rfe <- train_xgb("xgbLinear", train_set_rfe)
```

```{r}
plot(xgb_lin_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_lin_tuneGrid_rfe <- expand.grid(
  eta = 0.3,
  lambda = c(1e-04, 0),
  alpha = 1e-01,
  nrounds = seq(100, 200, 10)
)

xgb_lin_clf_rfe <- train_xgb(
  "xgbLinear", 
  train_set_rfe, 
  tuneGrid = xgb_lin_tuneGrid_rfe
)
```

```{r}
plot(xgb_lin_clf_rfe)
```

Tuning done.

```{r}
xgb_lin_metrics_rfe <- evaluate_model(
  xgb_lin_clf_rfe, 
  "XGB - Linear", 
  test_set_rfe,
  "XGB"
)
```

#### Support Vector Machine (Linear)

##### Train Set

```{r}
svm_lin_clf <- train_model("svmLinear2")
```

```{r}
plot(svm_lin_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
svm_lin_tuneGrid <- expand.grid(cost = seq(1, 5, 0.25))

svm_lin_clf <- train_model("svmLinear2", tuneGrid = svm_lin_tuneGrid)
```

```{r}
plot(svm_lin_clf)
```

Tuning done.

```{r}
svm_lin_metrics <- evaluate_model(svm_lin_clf, "SVM - Linear", family = "SVM")
```

##### Train Set (RFE)

```{r}
svm_lin_clf_rfe <- train_model("svmLinear2", train_set_rfe)
```

```{r}
plot(svm_lin_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
svm_lin_tuneGrid_rfe <- expand.grid(cost = seq(0.2, 1, 0.1))

svm_lin_clf_rfe <- train_model(
  "svmLinear2", 
  train_set_rfe, 
  tuneGrid = svm_lin_tuneGrid_rfe
)
```

```{r}
plot(svm_lin_clf_rfe)
```

Tuning done.

```{r}
svm_lin_metrics_rfe <- evaluate_model(
  svm_lin_clf_rfe, 
  "SVM - Linear", 
  test_set_rfe,
  "SVM"
)
```

#### Support Vector Machine (Polynomial)

##### Train Set

```{r}
svm_poly_clf <- train_model("svmPoly")
```

```{r}
plot(svm_poly_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
svm_poly_tuneGrid <- expand.grid(
  degree = 3:4,
  scale = 0.1,
  C = seq(0.25, 1, 0.05)
)

svm_poly_clf <- train_model("svmPoly", tuneGrid = svm_poly_tuneGrid)
```

```{r}
plot(svm_poly_clf)
```

Tuning done.

```{r}
svm_poly_metrics <- evaluate_model(
  svm_poly_clf, 
  "SVM - Polynomial", 
  family = "SVM"
)
```

##### Train Set (RFE)

```{r}
svm_poly_clf_rfe <- train_model("svmPoly", train_set_rfe)
```

```{r}
plot(svm_poly_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
svm_poly_tuneGrid_rfe <- expand.grid(
  degree = 3:4,
  scale = 0.1,
  C = seq(0.25, 1, 0.05)
)

svm_poly_clf_rfe <- train_model(
  "svmPoly", 
  train_set_rfe,
  tuneGrid = svm_poly_tuneGrid_rfe
)
```

```{r}
plot(svm_poly_clf_rfe)
```

Tuning done.

```{r}
svm_poly_metrics_rfe <- evaluate_model(
  svm_poly_clf_rfe, 
  "SVM - Polynomial", 
  test_set_rfe,
  "SVM"
)
```

#### Support Vector Machine (Radial Basis Function)

##### Train Set

```{r}
svm_rbf_clf <- train_model("svmRadial")
```

```{r}
plot(svm_rbf_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
svm_rbf_clf <- train_model("svmRadial", tuneLength = 15)
```

```{r}
plot(svm_rbf_clf)
```

Tuning done.

```{r}
svm_rbf_metrics <- evaluate_model(svm_rbf_clf, "SVM - RBF", family = "SVM")
```

##### Train Set (RFE)

```{r}
svm_rbf_clf_rfe <- train_model("svmRadial", train_set_rfe)
```

```{r}
plot(svm_rbf_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
svm_rbf_clf_rfe <- train_model("svmRadial", train_set_rfe, tuneLength = 15)
```

```{r}
plot(svm_rbf_clf_rfe)
```

Tuning done.

```{r}
svm_rbf_metrics_rfe <- evaluate_model(
  svm_rbf_clf_rfe, 
  "SVM - RBF", 
  test_set_rfe,
  "SVM"
)
```

### Evaluation

Set theme.

```{r}
theme_set(theme_bw())
```

Define color palette.

```{r}
custom_palette <- scale_fill_brewer(palette = "Pastel2")
```

Define a function to aggregate accuracy from model of same family.

```{r}
aggregate_accuracy <- function(df) {
  df_accuracy <- df %>%
    group_by(Family) %>%
    slice(which.max(Accuracy)) %>%
    arrange(desc(Accuracy)) %>%
    select(-1)

  print(df_accuracy)
  
  plot_accuracy <- df_accuracy %>%
    ggplot(aes(x = reorder(Family, -Accuracy), y = Accuracy, fill = Family)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = Accuracy), vjust=-0.5) +
    ggtitle("Accuracy by Model") +
    xlab("Model") +
    ylim(0, 1) +
    labs(fill = "Model") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    custom_palette
  
  print(plot_accuracy)
}
```

Define a function to aggregate recall score from model of same family.

```{r}
aggregate_recall_low <- function(df) {
  df_recall_low <- df %>%
    group_by(Family) %>%
    slice(which.max(Recall.Low)) %>%
    arrange(desc(Recall.Low)) %>%
    select(-1)

  print(df_recall_low)
  
  plot_recall_low <- df_recall_low %>%
    ggplot(aes(x = reorder(Family, -Recall.Low), y = Recall.Low, fill = Family)) +
    geom_bar(stat = "identity") +
    geom_text(aes(label = Recall.Low), vjust=-0.5) +
    ggtitle("Recall (Low Adaptivity) by Model") +
    xlab("Model") +
    ylab("Recall") +
    ylim(0, 1) +
    labs(fill = "Model") +
    theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) +
    custom_palette
  
  print(plot_recall_low)
}
```

#### Train Set

```{r}
df_eval <- rbind(
  lr_metrics,
  knn_metrics,
  nb_metrics,
  dt_metrics,
  rf_metrics,
  xgb_tree_metrics,
  xgb_lin_metrics,
  svm_lin_metrics,
  svm_poly_metrics,
  svm_rbf_metrics
)

df_eval
```

##### By Accuracy

```{r}
aggregate_accuracy(df_eval)
```

##### By Recall (Low Adaptivity)

```{r}
aggregate_recall_low(df_eval)
```

#### Train Set (RFE)

```{r}
df_eval_rfe <- rbind(
  lr_metrics_rfe,
  knn_metrics_rfe,
  nb_metrics_rfe,
  dt_metrics_rfe,
  rf_metrics_rfe,
  xgb_tree_metrics_rfe,
  xgb_lin_metrics_rfe,
  svm_lin_metrics_rfe,
  svm_poly_metrics_rfe,
  svm_rbf_metrics_rfe
)

df_eval_rfe
```

##### By Accuracy

```{r}
aggregate_accuracy(df_eval_rfe)
```

##### By Recall (Low Adaptivity)

```{r}
aggregate_recall_low(df_eval_rfe)
```

### Model Selection

Both K-Nearest Neighbors and Random Forest perform the best with all features selected.

We will choose K-Nearest Neighbors due to its simplicity.
