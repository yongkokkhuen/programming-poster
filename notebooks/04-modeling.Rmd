---
title: "04 - Modeling"
output: html_notebook
---

### Imports

```{r}
library(dplyr)
library(ggplot2)
library(caret)
```

```{r}
train_set <- readRDS("../data/train_set.rds")
head(train_set)
```

```{r}
test_set <- readRDS("../data/test_set.rds")
head(test_set)
```

```{r}
train_set_rfe <- readRDS("../data/train_set_rfe.rds")
head(train_set_rfe)
```

```{r}
test_set_rfe <- readRDS("../data/test_set_rfe.rds")
head(test_set_rfe)
```

### Modeling

Define repeated 10-fold cross validation.

```{r}
fit_control <- trainControl(
  method = "repeatedcv",
  number = 10,
  repeats = 10
)
```

Define a helper function to train model.

```{r}
train_model <- function(method = "",
                        data = train_set,
                        tuneGrid = NULL,
                        tuneLength = 3) {
  set.seed(42)

  fit <- train(
    Adaptivity.Level ~ .,
    data = data,
    method = method,
    trControl = fit_control,
    tuneGrid = tuneGrid,
    tuneLength = tuneLength
  )

  print(fit)
  
  return(fit)
}
```

Define a helper function to evaluate model.

```{r}
evaluate_model <- function(
    model = NULL, 
    alias = "",
    data = test_set,
    family = "") {
  
  pred <- predict(model, data)

  cm <- confusionMatrix(pred, data$Adaptivity.Level, mode = "prec_recall")
  print(cm)
  
  return(data.frame(
    Model = alias,
    Family = ifelse(family == "", alias, family),
    Accuracy = round(cm$overall[["Accuracy"]], 3),
    Precision.Low = round(cm$byClass["Class: Low", "Precision"], 3),
    Recall.Low = round(cm$byClass["Class: Low", "Recall"], 3),
    F1.Low = round(cm$byClass["Class: Low", "F1"], 3),
    Precision.Moderate = round(cm$byClass["Class: Moderate", "Precision"], 3),
    Recall.Moderate = round(cm$byClass["Class: Moderate", "Recall"], 3),
    F1.Moderate = round(cm$byClass["Class: Moderate", "F1"], 3),
    Precision.High = round(cm$byClass["Class: High", "Precision"], 3),
    Recall.High = round(cm$byClass["Class: High", "Recall"], 3),
    F1.High = round(cm$byClass["Class: High", "F1"], 3)
  ))
}
```

#### Logistic Regression

##### Train Set

```{r}
lr_clf <- train_model("glmnet")
```

```{r}
plot(lr_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
lr_clf <- train_model("glmnet", tuneLength = 10)
```

```{r}
plot(lr_clf)
```

Tuning done.

```{r}
lr_metrics <- evaluate_model(lr_clf, "LR")
```

##### Train Set (RFE)

```{r}
lr_clf_rfe <- train_model("glmnet", train_set_rfe)
```

```{r}
plot(lr_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
lr_clf_rfe <- train_model("glmnet", train_set_rfe, tuneLength = 10)
```

```{r}
plot(lr_clf_rfe)
```

Tuning done.

```{r}
lr_metrics_rfe <- evaluate_model(lr_clf_rfe, "LR", test_set_rfe)
```

#### K-Nearest Neighbors

##### Train Set

```{r}
knn_clf <- train_model("knn")
```

```{r}
plot(knn_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
knn_tuneGrid <- expand.grid(k = 1:5)

knn_clf <- train_model("knn", tuneGrid = knn_tuneGrid)
```

```{r}
plot(knn_clf)
```

Tuning done.

```{r}
knn_metrics <- evaluate_model(knn_clf, "KNN")
```

##### Train Set (RFE)

```{r}
knn_clf_rfe <- train_model("knn", train_set_rfe)
```

```{r}
plot(knn_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
knn_tuneGrid_rfe <- expand.grid(k = 1:5)

knn_clf_rfe <- train_model("knn", train_set_rfe, tuneGrid = knn_tuneGrid_rfe)
```

```{r}
plot(knn_clf_rfe)
```

Tuning done.

```{r}
knn_metrics_rfe <- evaluate_model(knn_clf_rfe, "KNN", test_set_rfe)
```

#### Naive Bayes

##### Train Set

```{r}
nb_clf <- train_model("naive_bayes")
```

```{r}
plot(nb_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
nb_tuneGrid <- expand.grid(
  adjust = 1,
  usekernel = TRUE,
  laplace = seq(0.1, 1, 0.1)
)

nb_clf <- train_model("naive_bayes", tuneGrid = nb_tuneGrid)
```

```{r}
plot(nb_clf)
```

Tuning done.

```{r}
nb_metrics <- evaluate_model(nb_clf, "NB")
```

##### Train Set (RFE)

```{r}
nb_clf_rfe <- train_model("naive_bayes", train_set_rfe)
```

```{r}
plot(nb_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
nb_tuneGrid_rfe <- expand.grid(
  adjust = 1,
  usekernel = TRUE,
  laplace = seq(0.1, 1, 0.1)
)

nb_clf_rfe <- train_model(
  "naive_bayes", 
  train_set_rfe, 
  tuneGrid = nb_tuneGrid_rfe
)
```

```{r}
plot(nb_clf_rfe)
```

Tuning done.

```{r}
nb_metrics_rfe <- evaluate_model(nb_clf_rfe, "NB", test_set_rfe)
```

#### Decision Tree

##### Train Set

```{r}
dt_clf <- train_model("rpart2")
```

```{r}
plot(dt_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
dt_clf <- train_model("rpart2", tuneLength = 10)
```

```{r}
plot(dt_clf)
```

Tuning done.

```{r}
dt_metrics <- evaluate_model(dt_clf, "DT")
```

##### Train Set (RFE)

```{r}
dt_clf_rfe <- train_model("rpart2", train_set_rfe)
```

```{r}
plot(dt_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
dt_clf_rfe <- train_model("rpart2", train_set_rfe, tuneLength = 10)
```

```{r}
plot(dt_clf_rfe)
```

Tuning done.

```{r}
dt_metrics_rfe <- evaluate_model(dt_clf_rfe, "DT", test_set_rfe)
```

#### Random Forest

##### Train Set

```{r}
rf_clf <- train_model("ranger")
```

```{r}
plot(rf_clf)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
rf_clf <- train_model("ranger", tuneLength = 10)
```

```{r}
plot(rf_clf)
```

Tuning done.

```{r}
rf_metrics <- evaluate_model(rf_clf, "RF")
```

##### Train Set (RFE)

```{r}
rf_clf_rfe <- train_model("ranger", train_set_rfe)
```

```{r}
plot(rf_clf_rfe)
```

Perform hyperparameter tuning using `tuneLength`.

```{r}
rf_clf_rfe <- train_model("ranger", train_set_rfe, tuneLength = 10)
```

```{r}
plot(rf_clf_rfe)
```

Tuning done.

```{r}
rf_metrics_rfe <- evaluate_model(rf_clf_rfe, "RF", test_set_rfe)
```
#### XGBoost (Tree)

Define a helper function to train XGBoost model with less verbose output.

```{r}
train_xgb <- function(method = "",
                        data = train_set,
                        tuneGrid = NULL,
                        tuneLength = 3) {
  set.seed(42)

  fit <- train(
    Adaptivity.Level ~ .,
    data = data,
    method = method,
    trControl = fit_control,
    tuneGrid = tuneGrid,
    tuneLength = tuneLength,
    verbosity = 0
  )

  print(fit)
  
  return(fit)
}
```

##### Train Set

```{r}
xgb_tree_clf <- train_xgb("xgbTree")
```

```{r}
plot(xgb_tree_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_tree_tuneGrid <- expand.grid(
  gamma = 0,
  min_child_weight = 1,
  max_depth = 3,
  eta = 0.4,
  subsample = seq(0.5, 1, 0.25),
  colsample_bytree = 0.8,
  nrounds = seq(100, 500, 10)
)

xgb_tree_clf <- train_xgb("xgbTree", tuneGrid = xgb_tree_tuneGrid)
```

```{r}
plot(xgb_tree_clf)
```

Tuning done.

```{r}
xgb_tree_metrics <- evaluate_model(xgb_tree_clf, "XGB - Tree", family = "XGB")
```

##### Train Set (RFE)

```{r}
xgb_tree_clf_rfe <- train_xgb("xgbTree", train_set_rfe)
```

```{r}
plot(xgb_tree_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_tree_tuneGrid_rfe <- expand.grid(
  gamma = 0,
  min_child_weight = 1,
  max_depth = 3,
  eta = 0.4,
  subsample = seq(0.5, 1, 0.25),
  colsample_bytree = 0.8,
  nrounds = seq(100, 500, 10)
)

xgb_tree_clf_rfe <- train_xgb(
  "xgbTree",  
  train_set_rfe,
  tuneGrid = xgb_tree_tuneGrid_rfe
)
```

```{r}
plot(xgb_tree_clf_rfe)
```

Tuning done.

```{r}
xgb_tree_metrics_rfe <- evaluate_model(
  xgb_tree_clf_rfe, 
  "XGB - Tree", 
  test_set_rfe,
  "XGB"
)
```

#### XGBoost (Linear)

##### Train Set

```{r}
xgb_lin_clf <- train_xgb("xgbLinear")
```

```{r}
plot(xgb_lin_clf)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_lin_tuneGrid <- expand.grid(
  eta = 0.3,
  lambda = c(1e-04, 1e-01, 0),
  alpha = 1e-04,
  nrounds = seq(100, 200, 10)
)

xgb_lin_clf <- train_xgb("xgbLinear", tuneGrid = xgb_lin_tuneGrid)
```

```{r}
plot(xgb_lin_clf)
```

Tuning done.

```{r}
xgb_lin_metrics <- evaluate_model(xgb_lin_clf, "XGB - Linear", family = "XGB")
```

##### Train Set (RFE)

```{r}
xgb_lin_clf_rfe <- train_xgb("xgbLinear", train_set_rfe)
```

```{r}
plot(xgb_lin_clf_rfe)
```

Perform hyperparameter tuning using `tuneGrid`.

```{r}
xgb_lin_tuneGrid_rfe <- expand.grid(
  eta = 0.3,
  lambda = c(1e-04, 0),
  alpha = 1e-01,
  nrounds = seq(100, 200, 10)
)

xgb_lin_clf_rfe <- train_xgb(
  "xgbLinear", 
  train_set_rfe, 
  tuneGrid = xgb_lin_tuneGrid_rfe
)
```

```{r}
plot(xgb_lin_clf_rfe)
```

Tuning done.

```{r}
xgb_lin_metrics_rfe <- evaluate_model(
  xgb_lin_clf_rfe, 
  "XGB - Linear", 
  test_set_rfe,
  "XGB"
)
```
